{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7156c2",
   "metadata": {},
   "source": [
    "**Dictionary Learning**<br>\n",
    "By: Laksh Patel<br><br>\n",
    "\n",
    "**Prerequisites:**<br>\n",
    "Facility in Linear Algebra<br>\n",
    "\n",
    "**Definitions:**<br>\n",
    "$X$ is the data matrix containing the data in $C(A)$. $D$ is the dictionary. $A$ is the sparse coefficent matrix. <br><br>\n",
    "\n",
    "**Food for Thought:**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e956d2",
   "metadata": {},
   "source": [
    "**Linear Regression**<br>\n",
    "By: Laksh Patel<br><br>\n",
    "\n",
    "**Prerequisites:**<br>\n",
    "Facility in Linear Algebra<br>\n",
    "Understanding of Numerical Analysis (Machine Epsilon, Big O notation, Floating Point Error) <br><br>\n",
    "\n",
    "**Definitions:**<br>\n",
    "$A$ is $m$ by $n$ matrix which contains the variables to best fit the observed values $b$. The $m$ rows contain all data points. The columns $n$ contain all features. <br><br>\n",
    "\n",
    "**Food for Thought:**<br>\n",
    "In search of a coefficent vector $x$ that satisfies $Ax=b$, we may realize that there exists no such $x$. This is because $b \\not\\in C(A)$. Often times, in real-world scenarios, this is true. Generally, when $A$ is over-determined ($m > n$), it has no solution $x$. Because the $x$ does not exist, we must instead look for the closest vector $b$ that allows the existance of an $x$. To do that, we must set $b'=\\mathrm{proj}_{C(A)}(b)$. There exists many ways of solving for the $x$ that satisfies $Ax = b'$, but some depend on the characteristics of $A$ and the precision of work. Also, directly computing the projection is extremely numerically unstable.\n",
    "\n",
    "If $A$ is full rank, then multiplying  $A^T$ on both sides, yielding $A^TAx=A^Tb$, provides a simple way of calculating for $x$, but it is not numerically stable. This is because when squaring small terms when computing $A^TA$, you may approach machine epsilon. \n",
    "\n",
    "Still considering the case of $A$ being full rank, we can utilize $QR$ factorization for a numerically stable solution (Q is orthogonal). \n",
    "$$A = QR \\\\\n",
    "Ax = b\\\\\n",
    "QRx = b\\\\\n",
    "Rx = Q^Tb \n",
    "$$\n",
    "$Q^T \\times b$ is numerically stable because $Q^T$ does not scale the space. Utilizing back-substitution, you can solve for $x$ with $R$ in $\\textit{O}(n^2)$ time. <br><br>\n",
    "**SVD's Application in Linear Regression:**<br>\n",
    "In many applications, however; $A$ <strong>won't have linearly independent columns</strong> or <strong>be square</strong>. This provides way for another way of finding an $x$ that solves $Ax=b'$. Singular Value Decomposition (abbreviated to SVD) isolates the orthogonal directions of variance, so it is particularly useful for data reduction. We can view the following insights: \n",
    "$$svd(A) = A = U\\Sigma V^T$$\n",
    "$$U,V \\text{ are unitary, so } \\quad U^T = U^{-1} \\quad \\& \\quad V^T = V^{-1}$$\n",
    "\n",
    "We can construct the psuedo-inverse $A^+$, by rewriting $A$.\n",
    "\n",
    "$$A = U\\Sigma V^T \\quad \\& \\quad Ax=b$$\n",
    "$$U\\Sigma V^Tx = b$$\n",
    "$$\\underbrace{V\\Sigma^-1U^TU\\Sigma V^T}_{I}x=V\\Sigma^-1U^Tb$$\n",
    "$$x=\\underbrace{V\\Sigma^-1U^T}_{A^+}b$$\n",
    "$$x=A^+b$$\n",
    "In doing this, we have inversed $\\Sigma$, which has only diaganol entries $\\sigma_i$. This makes the inverse computationally inexpensive and numerically stable. The fact that $U,V$ are unitary makes their indivisual inverse computation trivial as it is simply taking their respective transpose. So, this makes a psuedo-inverse of both singular and non-singular matrices. In practice, this is numerically stable and has application to matrices of all sizes. \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
