{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96466ea0",
   "metadata": {},
   "source": [
    "**Linear Regression**<br>\n",
    "By: Laksh Patel<br><br>\n",
    "\n",
    "**Prerequisites:**<br>\n",
    "Facility in Linear Algebra & Multi-Variable Calculus<br><br>\n",
    "\n",
    "**Intuition:**<br>\n",
    "We begin with the approximate system $A\\vec x\\approx \\vec b$, where $A_{m\\times n}$ contains $n$ feature columns and $m$ observations, $\\vec x$ is the coefficient vector, and $\\vec b$ is the vector of observed outputs. When $A\\vec x=\\vec b$ is exactly solvable, $\\vec b$ lies in the column space $C(A)$.<br><br>\n",
    "\n",
    "In general $\\vec b\\notin C(A)$, so the best attainable prediction is the orthogonal projection of $\\vec b$ onto $C(A)$, denoted $\\vec b'=\\mathrm{proj}_{C(A)}(\\vec b)$. The goal of linear regression is to find the $\\vec x$ for which $A\\vec x$ equals this projection.<br><br>\n",
    "\n",
    "To obtain such an $\\vec x$, define the least-squares loss $L(\\vec x)=\\|A\\vec x-\\vec b\\|^2$. Expanding, $\\|A\\vec x-\\vec b\\|^2=(A\\vec x-\\vec b)^T(A\\vec x-\\vec b)$, and differentiating gives $\\nabla L(\\vec x)=2A^T(A\\vec x-\\vec b)$. Setting this to zero produces the normal equations $A^TA\\,\\vec x=A^T\\vec b$.<br><br>\n",
    "\n",
    "The solution to these equations is given by the pseudoinverse formula $\\vec x=A^+\\vec b$, which yields the unique minimum-norm vector among all least-squares minimizers and is valid regardless of whether $A$ is full-rank or rank-deficient.<br><br>\n",
    "\n",
    "In all cases, $A\\vec x$ is the point in $C(A)$ closest to $\\vec b$ in Euclidean distance.\n",
    "\n",
    "**Application:**<br>\n",
    "Now that we've defined the mathematical ideology, let's think algorithmically.\n",
    "\n",
    "1. Take the data matrix $A$ and observation vector $\\vec b$.  \n",
    "2. Form the normal-equation components: compute $A^TA$ and $A^T\\vec b$.  \n",
    "3. Compute the pseudoinverse $A^+$ (typically via SVD).  \n",
    "4. Solve for the coefficients using $\\vec x=A^+\\vec b$.  \n",
    "5. Compute the prediction $\\hat b=A\\vec x$.  \n",
    "6. Compute the residual $r=\\vec b-\\hat b$ and its norm $\\|r\\|$ if needed.  \n",
    "7. Interpret $\\hat b$ as the projection of $\\vec b$ onto $C(A)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127dfdf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a25f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define some matrix operations\n",
    "\n",
    "def transpose(M):\n",
    "    return [list(row) for row in zip(*M)] #Swaps rows for columns\n",
    "\n",
    "def matmul(A, B):\n",
    "    # matrix * matrix or matrix * vector\n",
    "    if isinstance(B[0], list):\n",
    "        # A (m×n) * B (n×p)\n",
    "        return [[sum(a*b for a, b in zip(A_row, B_col))\n",
    "                 for B_col in zip(*B)]\n",
    "                for A_row in A]\n",
    "    else:\n",
    "        # A (m×n) * b (n)\n",
    "        return [sum(a*b for a, b in zip(A_row, B)) for A_row in A]\n",
    "\n",
    "def identity(n):\n",
    "    return [[1 if i==j else 0 for j in range(n)] for i in range(n)]\n",
    "\n",
    "def invert_matrix(M):\n",
    "    # Gauss–Jordan elimination\n",
    "    n = len(M)\n",
    "    A = [row[:] for row in M]\n",
    "    I = identity(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        # Pivot\n",
    "        pivot = A[i][i]\n",
    "        if pivot == 0:\n",
    "            raise ValueError(\"Matrix not invertible.\")\n",
    "        inv_pivot = 1 / pivot\n",
    "\n",
    "        # Normalize pivot row\n",
    "        for j in range(n):\n",
    "            A[i][j] *= inv_pivot\n",
    "            I[i][j] *= inv_pivot\n",
    "\n",
    "        # Eliminate other rows\n",
    "        for r in range(n):\n",
    "            if r != i:\n",
    "                factor = A[r][i]\n",
    "                for c in range(n):\n",
    "                    A[r][c] -= factor * A[i][c]\n",
    "                    I[r][c] -= factor * I[i][c]\n",
    "\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d6037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients x = [0.6666666666666679, 0.5]\n",
      "Prediction A x = [1.1666666666666679, 1.6666666666666679, 2.166666666666668]\n",
      "Residual = [-0.16666666666666785, 0.33333333333333215, -0.16666666666666785]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Linear Regression\n",
    "# -----------------------------\n",
    "\n",
    "def linear_regression(A, b):\n",
    "    A_T = transpose(A)                  # A^T\n",
    "    ATA = matmul(A_T, A)                # A^T A\n",
    "    ATb = matmul(A_T, b)                # A^T b  (vector)\n",
    "\n",
    "    # Solve (A^T A)x = A^T b\n",
    "    ATA_inv = invert_matrix(ATA)\n",
    "    x = matmul(ATA_inv, ATb)\n",
    "\n",
    "    # Prediction\n",
    "    b_hat = matmul(A, x)\n",
    "\n",
    "    # Residual\n",
    "    residual = [bi - hi for bi, hi in zip(b, b_hat)]\n",
    "\n",
    "    return x, b_hat, residual\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example\n",
    "# -----------------------------\n",
    "\n",
    "A = [\n",
    "    [1, 1],\n",
    "    [1, 2],\n",
    "    [1, 3]\n",
    "]\n",
    "\n",
    "b = [1, 2, 2]\n",
    "\n",
    "x, pred, r = linear_regression(A, b)\n",
    "\n",
    "print(\"Coefficients x =\", x)\n",
    "print(\"Prediction A x =\", pred)\n",
    "print(\"Residual =\", r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
